{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d728c9c",
   "metadata": {},
   "source": [
    "# Convert transcripts to readable files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8890fcdb",
   "metadata": {},
   "source": [
    "To work with this file, you should first obtain your transcription files from the [**KanjuTech Transcription and Diarization Model**](https://aws.amazon.com/marketplace/pp/prodview-ngtdx4ayt4emo), as demonstrated in this [sample notebook](https://github.com/KanjuTech/aws-marketplace/blob/main/KanjuTech-Transcription-Speaker-Diarization-Model.ipynb). Or you can use this [example output](https://github.com/KanjuTech/aws-marketplace/blob/main/example_output.json).\n",
    "\n",
    "> **Note**: This notebook contains elements that render correctly in the Jupyter interface. Open it from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "\n",
    "JSON description:\n",
    "\n",
    "**id** - Name of transcribed audio file.  \n",
    "**speaker** - Speaker number.  \n",
    "**start** - Phrase start time in seconds.  \n",
    "**end** - Phrase end time in seconds.  \n",
    "**text** - Text of the phrase.  \n",
    "\n",
    "> **Note**: This reference notebook cannot run unless you make the suggested changes in the notebook.\n",
    "\n",
    "## Contents:\n",
    "1. [Phrase by phrase](#1.-Phrase-by-phrase)\n",
    "2. [Speaker by speaker](#2.-Speaker-by-speaker)\n",
    "3. [SRT](#3.-SRT)\n",
    "4. [Clean-up JSON directory](#4.-Clean-up-JSON-directory)\n",
    "5. [Questions](#5.-Questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d1d406",
   "metadata": {},
   "source": [
    "If you are converting transcriptions to subtitle files for the first time, you must install the SRT library first. To do this, use the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fa1a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U srt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3593b44d",
   "metadata": {},
   "source": [
    "After installing the library, restart the kernel and delete the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1297c480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "import textwrap\n",
    "import srt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f0b6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 's3://<Name-of-your-existing-S3-bucket>' # Write the name of your S3 bucket where you store your input files and want to save the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14f28ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify S3 folders\n",
    "json_outputs = bucket+'/'+'batch-transcript' # Your folder with results of transcription from the model\n",
    "txt_transcripts = bucket+'/'+'final-transcript' # Folder for converted transcriptions\n",
    "srt_folder = bucket+'/'+'srt' # Folder for SRT files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c00f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem()\n",
    "fs_ls = fs.ls(json_outputs)\n",
    "paths = list(filter(lambda k: '.' in k, fs_ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c145163f",
   "metadata": {},
   "source": [
    "## 1. Phrase by phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d0c81c",
   "metadata": {},
   "source": [
    "This script will convert the transcript into text, segmented into phrases, without regard to the speaker who uttered them:\n",
    "\n",
    "> Speaker_1 (0:00:00): Okay, how are you?  \n",
    "Speaker_2 (0:00:02): I'm pretty good.  \n",
    "Speaker_2 (0:00:03): That's a strange deal.  \n",
    "Speaker_2 (0:00:05): What's that all about?  \n",
    "Speaker_1 (0:00:06): Well, you know, I'm on a computer mailing list on my e-mail.  \n",
    "Speaker_1 (0:00:14): It's a research thing for psycholinguistics.  \n",
    "Speaker_1 (0:00:17): And at UPenn, they're building a linguistic database of many languages, and so they were offering free phone calls anywhere in the world.  \n",
    "Speaker_1 (0:00:27): We have to only speak one language, though.  \n",
    "Speaker_1 (0:00:30): So they're collecting lots of different languages, but you have to only speak the two parties have to speak the same language.  \n",
    "Speaker_1 (0:00:37): So I could only call a native English speaker.  \n",
    "Speaker_1 (0:00:40): So that was the deal.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7014e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert json files and save to txt\n",
    "for json_file_path in paths:\n",
    "    # Load json from s3\n",
    "    with fs.open(json_file_path, \"r\") as f:\n",
    "        output = f.read()\n",
    "        contents = json.loads(output)\n",
    "        \n",
    "    # Convert and save\n",
    "    file_name = os.path.splitext(os.path.split(json_file_path)[-1])[0]\n",
    "    with fs.open(txt_transcripts+'/{}.txt'.format(file_name), 'w') as f:\n",
    "        try:\n",
    "            for content in contents:\n",
    "                print(content[\"speaker\"], \n",
    "                      '({}):'.format(datetime.timedelta(seconds=round(content[\"start\"]))), \n",
    "                      content[\"text\"], \n",
    "                      file=f)\n",
    "        except:\n",
    "            print(contents[0], file=f)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d655e3",
   "metadata": {},
   "source": [
    "## 2. Speaker by speaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3fe8ff",
   "metadata": {},
   "source": [
    "This script will convert the transcript into text, segmented into speakers. If a speaker utters multiple phrases consecutively, they will be merged:\n",
    "\n",
    "> Speaker_1 (0:00:00): Okay, how are you?  \n",
    "Speaker_2 (0:00:02): I'm pretty good. That's a strange deal. What's that all about?  \n",
    "Speaker_1 (0:00:06): Well, you know, I'm on a computer mailing list on my e-mail. It's a research thing for psycholinguistics. And at UPenn, they're building a linguistic database of many languages, and so they were offering free phone calls anywhere in the world. We have to only speak one language, though. So they're collecting lots of different languages, but you have to only speak the two parties have to speak the same language. So I could only call a native English speaker. So that was the deal.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc91020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for json_file_path in paths:\n",
    "    # Load json from s3\n",
    "    with fs.open(json_file_path, \"r\") as f:\n",
    "        output = f.read()\n",
    "        contents = json.loads(output)\n",
    "        \n",
    "    # Convert and save\n",
    "    file_name = os.path.splitext(os.path.split(json_file_path)[-1])[0]\n",
    "    with fs.open(txt_transcripts+'/{}.txt'.format(file_name), 'w') as f:\n",
    "        try:\n",
    "            current_speaker = contents[0]['speaker']\n",
    "            text = ''\n",
    "            s_time = contents[0][\"start\"]\n",
    "            for content in contents:\n",
    "                speaker = content['speaker']\n",
    "                if current_speaker == speaker:\n",
    "                    text += content[\"text\"]\n",
    "                else:\n",
    "                    print(current_speaker, \n",
    "                          '({}):'.format(datetime.timedelta(seconds=round(s_time))), \n",
    "                          text, \n",
    "                          file=f)\n",
    "                    text = ''\n",
    "                    s_time = content[\"start\"]\n",
    "                    text += content[\"text\"]\n",
    "                    current_speaker = speaker\n",
    "            print(current_speaker, \n",
    "                  '({}):'.format(datetime.timedelta(seconds=round(s_time))), \n",
    "                  text, \n",
    "                  file=f)\n",
    "        except:\n",
    "            print(contents[0], file=f)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e734011d",
   "metadata": {},
   "source": [
    "## 3. SRT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef2b0fc",
   "metadata": {},
   "source": [
    "This script will convert the transcripts to SRT files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b73a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the max number of lines for each subtitle\n",
    "max_lines = 3 # Max number of lines for each sabtitle\n",
    "\n",
    "# Set the max number of characters for each line\n",
    "# Note that character_limits will be increased for subs where phrase text > (character_limits * max_lines)\n",
    "character_limits = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66719f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for json_file_path in paths:\n",
    "    # Load json from s3\n",
    "    with fs.open(json_file_path, \"r\") as f:\n",
    "        output = f.read()\n",
    "        contents = json.loads(output)\n",
    "        \n",
    "    # Convert and save\n",
    "    file_name = os.path.splitext(os.path.split(json_file_path)[-1])[0]\n",
    "    subs = []\n",
    "    with fs.open(srt_folder+'/{}.srt'.format(file_name), 'w') as f:\n",
    "        try:\n",
    "            for idx, content in enumerate(contents):\n",
    "                if len(content['text']) <= (character_limits * max_lines):\n",
    "                    num_lines = len(content['text']) // character_limits + 1\n",
    "                else:\n",
    "                    num_lines = max_lines\n",
    "                text = ' '.join(textwrap.wrap(content['text'],\n",
    "                                              len(content['text'])/num_lines+10, \n",
    "                                              break_long_words=False, \n",
    "                                              replace_whitespace=False, \n",
    "                                              subsequent_indent='\\n'))\n",
    "                subs.append(srt.Subtitle(idx+1,\n",
    "                                         datetime.timedelta(seconds=content['start']),\n",
    "                                         datetime.timedelta(seconds=content['end']),\n",
    "                                         text))\n",
    "            print(srt.compose(subs), file=f)\n",
    "        except:\n",
    "            print(contents[0], file=f)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c59dfe",
   "metadata": {},
   "source": [
    "## 4. Clean-up JSON directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65a3627",
   "metadata": {},
   "source": [
    "After converting JSON files to .txt, you can remove files if you don't need them anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572850c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_ls = fs.ls(json_outputs)\n",
    "paths = list(filter(lambda k: '.' in k, fs_ls))\n",
    "for file in paths:\n",
    "    fs.rm(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00840dd2",
   "metadata": {},
   "source": [
    "## 5. Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b2ebe1",
   "metadata": {},
   "source": [
    "If you have any questions about our product, feel free to email us at aws@kanju.tech or schedule a [meeting](https://calendly.com/kanjutech)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
