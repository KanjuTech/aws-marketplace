{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy KanjuTech Transcription and Speaker Diarization Model Package from AWS Marketplace "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KanjuTech's Transcription and Diarization model ensures secure end-to-end recognition of multi-participant conversations. The model efficiently handles over 12 hours of recording in just one hour on ml.p3.2xlarge. The Stable version of the model supports 10 languages with human-level accuracy (WER 3-8%). The Confusion Error Rate (CER) for audio with 6+ speakers is 2.2%.\n",
    "\n",
    "The Release Candidate version supports an additional 19 languages with lower and unstable quality.\n",
    "\n",
    "This sample notebook shows you how to deploy the [**KanjuTech Transcription and Diarization Model**](https://aws.amazon.com/marketplace/pp/prodview-ngtdx4ayt4emo) using Amazon SageMaker.\n",
    "\n",
    "> **Note**: This reference notebook cannot run unless you make the suggested changes in the notebook.\n",
    "\n",
    "## Pre-requisites:\n",
    "1. **Note**: This notebook contains elements that render correctly in the Jupyter interface. Open it from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that the IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. To deploy this ML model successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions, and you have the authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "    2. your AWS account has a [**KanjuTech Transcription and Diarization Model**](https://aws.amazon.com/marketplace/pp/prodview-ngtdx4ayt4emo) subscription. If so, skip the step: [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "\n",
    "## Contents:\n",
    "1. [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "2. [Create an endpoint and perform real-time inference](#2.-Create-an-endpoint-and-perform-real-time-inference)\n",
    "    1. [Create an endpoint](#A.-Create-an-endpoint)\n",
    "    2. [Create input payload](#B.-Create-input-payload)\n",
    "    3. [Perform real-time inference](#C.-Perform-real-time-inference)\n",
    "    4. [Delete endpoint and model](#D.-Delete-endpoint-and-model)\n",
    "3. [Perform batch inference](#3.-Perform-batch-inference) \n",
    "    1. [Create encoded input data](#A.-Create-encoded-input-data)\n",
    "    2. [Run batch transform job](#B.-Run-batch-transform-job)\n",
    "    3. [Delete encoded input data](#C.-Delete-encoded-input-data)\n",
    "    4. [Delete the model](#D.-Delete-the-model)\n",
    "4. [Visualize output](#4.-Visualize-output)\n",
    "5. [Release Candidate version (optional)](#5.-Release-Candidate-version-(optional))\n",
    "6. [Troubleshooting](#6.-Troubleshooting)\n",
    "7. [Questions](#7.-Questions)\n",
    "    \n",
    "We recommend using ml.p3.2xlarge instance for real-time and batch inference.\n",
    "\n",
    "The maximum audio file size for real-time inference is 15MB, and for batch transform job is 75MB for each file. The duration limits for one audio file for real-time inference are about:\n",
    "- 7 mins on ml.g4dn.xlarge,\n",
    "- 11 mins on ml.p3.2xlarge.\n",
    "\n",
    "It's no duration limits batch inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Subscribe to the model package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To subscribe to the model package:\n",
    "1. Open the model package listing page [**KanjuTech Transcription and Diarization Model**](https://aws.amazon.com/marketplace/pp/prodview-ngtdx4ayt4emo).\n",
    "1. On the AWS Marketplace listing, click on the **Continue to subscribe** button.\n",
    "1. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you and your organization agree with EULA, pricing, and support terms. \n",
    "1. Once you click on the **Continue to configuration** button and choose a **region**, you will see a **Product Arn** displayed. This is the model package ARN that you need to specify while creating a deployable model using Boto3. Copy the ARN corresponding to your region and specify it in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-03T17:30:41.123716Z",
     "start_time": "2024-03-03T17:30:41.100284Z"
    }
   },
   "outputs": [],
   "source": [
    "model_package_arn = \"<Specify the Model package ARN that corresponds to your AWS region>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "from sagemaker import ModelPackage\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import s3fs\n",
    "import base64\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "\n",
    "sagemaker_session = sage.Session()\n",
    "\n",
    "bucket = 's3://<Name-of-your-existing-S3-bucket>' # Write the name of your S3 bucket where you store your input files and want to save the output\n",
    "runtime = boto3.client(\"runtime.sagemaker\")\n",
    "\n",
    "real_time_content_type = \"application/json\"\n",
    "batch_transform_content_type = \"application/json\"\n",
    "accept = \"application/json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create an endpoint and perform real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html) if you want to understand how real-time inference with Amazon SageMaker works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Create an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"kanjutech-transcription-speaker-diarization\" # Write the endpoint name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify instance type\n",
    "real_time_inference_instance_type = \"ml.p3.2xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  **Note**: We recommend using ml.p3.2xlarge instance for real-time inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deployable model from the model package.\n",
    "model = ModelPackage(role=role, model_package_arn=model_package_arn, sagemaker_session=sagemaker_session)\n",
    "\n",
    "# Deploy the model\n",
    "predictor = model.deploy(1, real_time_inference_instance_type, endpoint_name=model_name)\n",
    "\n",
    "# Wait until it prints \"!\" after \"----------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the endpoint has been created, you can perform real-time inference.\n",
    "\n",
    "If you get an error here, please see the [Troubleshooting](#6.-Troubleshooting).\n",
    "\n",
    "**WARNING!** \n",
    "\n",
    "**Remember to** [**Delete your endpoint and resources**](#D.-Delete-endpoint-and-model) whenever you finish your work with real-time inference to stop incurring your charges!\n",
    "\n",
    "For more information, please visit this [page](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-delete-resources.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Create input payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input audio files from the S3 bucket and encode in base64. The maximum audio file size for real-time inference is 15MB for each file. \n",
    "\n",
    "The duration limits for one audio file for real-time inference are about:\n",
    "- 7 mins on ml.g4dn.xlarge,\n",
    "- 11 mins on ml.p3.2xlarge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify S3 folders\n",
    "endpoint_input = bucket+'/'+'endpoint-audio' # Your folder on the S3 bucket where you store input audio\n",
    "endpoint_output = bucket+'/'+'endpoint-transcript' # Folder for results of real-time transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem()\n",
    "fs_ls = fs.ls(endpoint_input)\n",
    "paths = list(filter(lambda k: '.' in k, fs_ls))\n",
    "\n",
    "# For this example, we encode only one file from paths\n",
    "input_file_path = paths[0]\n",
    "\n",
    "with fs.open(input_file_path, \"rb\") as f:\n",
    "    edata = base64.b64encode(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create JSON input with encoded data and transcription parameters.\n",
    "\n",
    "**language** (*str*) - Use \"auto\" to automatically detect the language or specify the language code:\n",
    "- English: \"en\"\n",
    "- Spanish: \"es\"\n",
    "- French: \"fr\"\n",
    "- Portuguese: \"pt\"\n",
    "- Russian: \"ru\"\n",
    "- Indonesian: \"id\"\n",
    "- German: \"de\"\n",
    "- Japanese: \"ja\"\n",
    "- Turkish: \"tr\"\n",
    "- Italian: \"it\"\n",
    "\n",
    "**num_speakers** (*int* or *str*) - Use \"auto\" to automatically identify the number of speakers or specify the exact number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify audio parameters\n",
    "language = \"en\"\n",
    "num_speakers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = {}\n",
    "json_file['file'] = edata.decode('utf-8')\n",
    "json_file['language'] = language\n",
    "json_file['num_speakers'] = num_speakers\n",
    "json_file['f_name'] = os.path.basename(input_file_path)\n",
    "data = json.dumps(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Perform real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke the endpoint for real-time inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = runtime.invoke_endpoint(\n",
    "    EndpointName=model_name,\n",
    "    Body=data, \n",
    "    ContentType=real_time_content_type,  \n",
    "    Accept=accept,  \n",
    ")\n",
    "\n",
    "# Save the transcript to S3\n",
    "with fs.open(endpoint_output+'/'+json_file['f_name'].split('.')[0]+'_'+language+'_'+str(num_speakers)+'.json', \"wb\") as f:\n",
    "    f.write((results['Body']).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Delete endpoint and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully performed a real-time inference, you no longer need the endpoint. You can terminate the endpoint to avoid being charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sagemaker_session.delete_endpoint(model_name)\n",
    "model.sagemaker_session.delete_endpoint_config(model_name)\n",
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING!** \n",
    "\n",
    "**Remember to** [**Delete your endpoint and resources**](#D.-Delete-endpoint-and-model) whenever you finish your work with real-time inference to stop incurring your charges!\n",
    "\n",
    "For more information, please visit this [page](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-delete-resources.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perform batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html) if you want to understand how batch inference with Amazon SageMaker works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Create encoded input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload input files to S3 for batch transform job. The maximum audio file size for the batch transform job is 75MB for each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify S3 folders\n",
    "batch_input = bucket+'/'+'batch-audio' # Your folder on the S3 bucket where you store input audio\n",
    "batch_encode = bucket+'/'+'batch-audio-encode' # Folder for encoded audio files\n",
    "batch_output = bucket+'/'+'batch-transcript' # Folder for results of batch transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create JSON files with encoded data and transcription parameters.\n",
    "\n",
    "**language** (*str*) - Use \"auto\" to automatically detect the language or specify the language code:\n",
    "- English: \"en\"\n",
    "- Spanish: \"es\"\n",
    "- French: \"fr\"\n",
    "- Portuguese: \"pt\"\n",
    "- Russian: \"ru\"\n",
    "- Indonesian: \"id\"\n",
    "- German: \"de\"\n",
    "- Japanese: \"ja\"\n",
    "- Turkish: \"tr\"\n",
    "- Italian: \"it\"\n",
    "\n",
    "**num_speakers** (*int* or *str*) - Use \"auto\" to automatically identify the number of speakers or specify the exact number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify batch parameters\n",
    "language = \"auto\"\n",
    "num_speakers = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem()\n",
    "fs_ls = fs.ls(batch_input)\n",
    "paths = list(filter(lambda k: '.' in k, fs_ls))\n",
    "for input_file_path in paths:\n",
    "    with fs.open('s3://' + input_file_path, \"rb\") as f:\n",
    "        edata = base64.b64encode(f.read())\n",
    "    json_file = {}\n",
    "    json_file['file'] = edata.decode('utf-8')\n",
    "    json_file['language'] = language\n",
    "    json_file['num_speakers'] = num_speakers\n",
    "    json_file['f_name'] = os.path.basename(input_file_path)\n",
    "    data = json.dumps(json_file)\n",
    "    with fs.open(batch_encode+'/'+json_file['f_name'].split('.')[0]+'_'+language+'_'+str(num_speakers)+'.json', \"w\") as f:\n",
    "        f.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Run batch transform job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify instance type\n",
    "batch_transform_inference_instance_type = \"ml.p3.2xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  **Note**: We recommend using ml.p3.2xlarge instance for batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deployable model from the model package.\n",
    "model = ModelPackage(role=role, model_package_arn=model_package_arn, sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = model.transformer(1, \n",
    "                                batch_transform_inference_instance_type, \n",
    "                                output_path=batch_output, \n",
    "                                accept=accept, \n",
    "                                max_payload=100)\n",
    "transformer.transform(batch_encode, content_type=batch_transform_content_type)\n",
    "transformer.wait()\n",
    "\n",
    "# Wait until it prints text after \"..........\" and finishes the batch inference after that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get an error here, please see the [Troubleshooting](#6.-Troubleshooting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Delete encoded input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully performed a batch inference, you no longer need the encoded input files. Clean up the folder to prevent processing finished data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_ls = fs.ls(batch_encode)\n",
    "paths = list(filter(lambda k: '.' in k, fs_ls))\n",
    "for file in paths:\n",
    "    fs.rm(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore examples of visualization and converting transcription, see this [file](https://github.com/KanjuTech/aws-marketplace/blob/main/results_converter.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Release Candidate version (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Release Candidate version supports additional languages with lower and unstable quality. If you want to access these languages, please specify the model version in your request, as in the example.\n",
    "\n",
    "Additional languages and codes:\n",
    "- Chinese: \"zh\"\n",
    "- Vietnamese: \"vi\"\n",
    "- Tagalog: \"tl\"\n",
    "- Korean: \"ko\"\n",
    "- Thai: \"th\"\n",
    "- Polish: \"pl\"\n",
    "- Ukrainian: \"uk\"\n",
    "- Dutch: \"nl\"\n",
    "- Romanian: \"ro\"\n",
    "- Hungarian: \"hu\"\n",
    "- Greek: \"el\"\n",
    "- Swedish: \"sv\"\n",
    "- Czech: \"cs\"\n",
    "- Bulgarian: \"bg\"\n",
    "- Slovak: \"sk\"\n",
    "- Croatian: \"hr\"\n",
    "- Danish: \"da\"\n",
    "- Finnish: \"fi\"\n",
    "- Norwegian: \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify parameters\n",
    "language = \"pl\"\n",
    "num_speakers = \"auto\"\n",
    "model_version = \"rc\" # \"stable\" or \"rc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = {}\n",
    "json_file['file'] = edata.decode('utf-8')\n",
    "json_file['language'] = language\n",
    "json_file['num_speakers'] = num_speakers\n",
    "json_file['f_name'] = os.path.basename(input_file_path)\n",
    "json_file['version'] = model_version # Optional. If not specified, input will be processed using the Stable version.\n",
    "data = json.dumps(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cannot create already existing endpoint configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This error occurs when the user interrupts the inference deployment and tries to rerun it. To restart the deployment, first delete the previously created configurations. You can find this command in the [Delete endpoint and model](#D.-Delete-endpoint-and-model) cell.\n",
    "\n",
    "Please wait for the deployment to complete. This process may take several minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResourceLimitExceeded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you receive an error due to the lack of a quota for your instance type, you can increase it by sending a request:\n",
    "1. Open the **Amazone SageMaker** [**Service Quotas**](https://console.aws.amazon.com/servicequotas/home/services/sagemaker/quotas) page.\n",
    "2. Filter **Service quotas** by \"ml.p3.2xlarge for endpoint usage\" for real-time inference or by \"ml.p3.2xlarge for transform job usage\" for batch inference.\n",
    "3. Select and click on the **Request increase at account-level** button.\n",
    "4. Enter the total amount you want the quota to be and click the **Request** button.\n",
    "5. Wait until AWS Support increases your quotas for this instance type.\n",
    "\n",
    "> **Note**: To speed up the processing of your request, please indicate in your correspondence with AWS Support that this type of instance is required for this product.\n",
    "\n",
    "For more information about requesting a quota increase, visit this [page](https://docs.aws.amazon.com/servicequotas/latest/userguide/request-quota-increase.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have any questions about our product, feel free to email us at aws@kanju.tech or schedule a [meeting](https://calendly.com/kanjutech)."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
